{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.initializers import GlorotUniform, RandomNormal\n",
    "from tensorflow.keras.models import load_model\n",
    "print (\"TensorFlow version: \" + tf.__version__)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset\n",
    " Each File has 31 rows and 4 columns. The first 3 columns indicate heart rate, respiratory rate, and oxygen saturation of a baby; the last column indicates if the baby is in pain or not (0 means no pain, 1 means slight pain, 2 means severe pain, # means label is missing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./Data/\"\n",
    "csv_files = glob.glob(data_dir + \"/**/*.csv\", recursive=True) # get all csv files in the data directory\n",
    "combined_csv = pd.concat([pd.read_csv(f, header=None) for f in csv_files], ignore_index=True) # combine all csv files into one dataframe\n",
    "combined_csv.drop(columns=[0], inplace=True) # drop the first column (index)\n",
    "combined_csv = combined_csv[combined_csv.iloc[:, 3] != \"#\"] # remove rows with \"#\" in the 4th column\n",
    "combined_csv = combined_csv[combined_csv.iloc[:, 2] != \"--1\"] # remove row with \"--1\" in the 3rd column\n",
    "combined_csv # display the combined dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Shuffle dataset\n",
    "combined_csv = shuffle(combined_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset\n",
    "train, test = train_test_split(combined_csv, test_size=0.1) \n",
    "train, valid = train_test_split(combined_csv, test_size=0.1) # split the dataset into 90% training and 10% validation (test) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Dataset\n",
    "scaler = StandardScaler() # initialize scaler\n",
    "train_x = scaler.fit_transform(train.iloc[:, 0:3]) # fit the scaler on the training data and transform the training data\n",
    "train_y = train.iloc[:, 3] # get the labels for the training data\n",
    "valid_x = scaler.transform(valid.iloc[:, 0:3]) # transform the validation data\n",
    "valid_y = valid.iloc[:, 3] # get the labels for the validation data\n",
    "test_x = scaler.transform(test.iloc[:, 0:3]) # transform the test data\n",
    "test_y = test.iloc[:, 3] # get the labels\n",
    "\n",
    "#  Non-Normalized Dataset\n",
    "# train_x = train.iloc[:, 0:3].values\n",
    "# train_y = train.iloc[:, 3].values   \n",
    "# valid_x = valid.iloc[:, 0:3].values\n",
    "# valid_y = valid.iloc[:, 3].values\n",
    "# test_x = test.iloc[:, 0:3].values\n",
    "# test_y = test.iloc[:, 3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to float32\n",
    "train_x = np.array(train_x, np.float32)\n",
    "train_y = np.array(train_y, np.float32)\n",
    "valid_x = np.array(valid_x, np.float32)\n",
    "valid_y = np.array(valid_y, np.float32)\n",
    "test_x = np.array(test_x, np.float32)\n",
    "test_y = np.array(test_y, np.float32)\n",
    "\n",
    "\n",
    "# print(train_y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "# Xavier w/ regularizer\n",
    "# model = Sequential([\n",
    "#     Input(shape=(3,)),\n",
    "#     Dense(32, activation='relu', kernel_initializer=GlorotUniform(), kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     Dense(64, activation='relu', kernel_initializer=GlorotUniform(), kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     Dense(32, activation='relu', kernel_initializer=GlorotUniform(), kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     Dense(16, activation='relu', kernel_initializer=GlorotUniform(), kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     Dense(3, activation='softmax', kernel_initializer=GlorotUniform(), kernel_regularizer=regularizers.l2(0.01))\n",
    "# ])\n",
    "\n",
    "# Xavier w/o regularizer\n",
    "# model = Sequential([\n",
    "#     Input(shape=(3,)),\n",
    "#     Dense(32, activation='relu', kernel_initializer=GlorotUniform()),\n",
    "#     Dense(64, activation='relu', kernel_initializer=GlorotUniform()),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(32, activation='relu', kernel_initializer=GlorotUniform()),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(16, activation='relu', kernel_initializer=GlorotUniform()),\n",
    "#     Dense(3, activation='softmax', kernel_initializer=GlorotUniform())\n",
    "# ])\n",
    "\n",
    "# Random Normal w/ regularizer\n",
    "# model = Sequential([\n",
    "#     Input(shape=(3,)),\n",
    "#     Dense(32, activation='relu', kernel_initializer=RandomNormal(), kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     Dense(64, activation='relu', kernel_initializer=RandomNormal(), kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     Dense(32, activation='relu', kernel_initializer=RandomNormal(), kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     Dense(16, activation='relu', kernel_initializer=RandomNormal(), kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     Dense(3, activation='softmax', kernel_initializer=RandomNormal(), kernel_regularizer=regularizers.l2(0.01))\n",
    "# ])\n",
    "\n",
    "# Random Normal w/o regularizer\n",
    "model = Sequential([\n",
    "    Input(shape=(3,)),\n",
    "    Dense(32, activation='relu', kernel_initializer=RandomNormal()), #\n",
    "    Dense(64, activation='relu', kernel_initializer=RandomNormal()),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu', kernel_initializer=RandomNormal()),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu', kernel_initializer=RandomNormal()),\n",
    "    Dense(3, activation='softmax', kernel_initializer=RandomNormal())\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers and Compiling\n",
    "\n",
    "# ADAM Optimizer w/ Learning Rate of 0.1\n",
    "# model.compile(optimizer=Adam(learning_rate=.1), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ADAM Optimizer w/ Learning Rate of 0.001\n",
    "# model.compile(optimizer=Adam(learning_rate=.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Momentum Optimizer w/ Learning Rate of 0.1\n",
    "# model.compile(optimizer=SGD(learning_rate=.1, momentum=0.7), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Momentum Optimizer w/ Learning Rate of 0.001\n",
    "model.compile(optimizer=SGD(learning_rate=.001, momentum=0.7), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ReduceLROnPlateau callback,hopes in reducing plateu in loss\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "\n",
    "# Batch Size of 32, Epochs of 80\n",
    "history = model.fit(train_x, train_y, epochs=80, batch_size=32, validation_data=(valid_x, valid_y), callbacks=[reduce_lr])\n",
    "\n",
    "# Batch size of 100, epochs of 80\n",
    "# history = model.fit(train_x, train_y, epochs=80, batch_size=100, validation_data=(valid_x, valid_y), callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Evaluation\n",
    "test_loss, test_acc = model.evaluate(valid_x, valid_y)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Script to Save and Load Moel for Testing\n",
    "model.save('models/neural_network_model.keras') # save model\n",
    "loaded_model = load_model('models/neural_network_model.keras') # load model\n",
    "\n",
    "# Evaluate on validation set\n",
    "test_loss, test_acc = loaded_model.evaluate(valid_x, valid_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Results\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label = \"Train Loss\")\n",
    "plt.plot(history.history['val_loss'], label = \"Validation Loss\")\n",
    "plt.axhline(y=test_loss, color='r', linestyle='--', label='Test Loss') # plot test loss as horizontal line\n",
    "plt.title('Loss over Epochs ')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
