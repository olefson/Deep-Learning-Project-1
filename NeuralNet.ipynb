{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.initializers import GlorotUniform, RandomNormal\n",
    "from tensorflow.keras.models import load_model\n",
    "print (\"TensorFlow version: \" + tf.__version__)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset\n",
    " Each File has 31 rows and 4 columns. The first 3 columns indicate heart rate, respiratory rate, and oxygen saturation of a baby; the last column indicates if the baby is in pain or not (0 means no pain, 1 means slight pain, 2 means severe pain, # means label is missing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./Data/\"\n",
    "csv_files = glob.glob(data_dir + \"/**/*.csv\", recursive=True) # get all csv files in the data directory\n",
    "combined_csv = pd.concat([pd.read_csv(f, header=None) for f in csv_files], ignore_index=True) # combine all csv files into one dataframe\n",
    "combined_csv.drop(columns=[0], inplace=True) # drop the first column (index)\n",
    "combined_csv = combined_csv[combined_csv.iloc[:, 3] != \"#\"] # remove rows with \"#\" in the 4th column\n",
    "combined_csv = combined_csv[combined_csv.iloc[:, 2] != \"--1\"] # remove row with \"--1\" in the 3rd column (why is that there?)\n",
    "combined_csv # display the combined dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Shuffle dataset\n",
    "combined_csv = shuffle(combined_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset\n",
    "train, test = train_test_split(combined_csv, test_size=0.1) \n",
    "train, valid = train_test_split(combined_csv, test_size=0.1) # split the dataset into 90% training and 10% validation (test) sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Option: Normalization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Dataset\n",
    "scaler = StandardScaler() # initialize scaler\n",
    "train_x = scaler.fit_transform(train.iloc[:, 0:3]) # fit the scaler on the training data and transform the training data\n",
    "train_y = train.iloc[:, 3] # get the labels for the training data\n",
    "valid_x = scaler.transform(valid.iloc[:, 0:3]) # transform the validation data\n",
    "valid_y = valid.iloc[:, 3] # get the labels for the validation data\n",
    "test_x = scaler.transform(test.iloc[:, 0:3]) # transform the test data\n",
    "test_y = test.iloc[:, 3] # get the labels\n",
    "\n",
    "#  Non-Normalized Dataset\n",
    "# train_x = train.iloc[:, 0:3].values\n",
    "# train_y = train.iloc[:, 3].values   \n",
    "# valid_x = valid.iloc[:, 0:3].values\n",
    "# valid_y = valid.iloc[:, 3].values\n",
    "# test_x = test.iloc[:, 0:3].values\n",
    "# test_y = test.iloc[:, 3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to float32 (this was annoying)\n",
    "train_x = np.array(train_x, np.float32)\n",
    "train_y = np.array(train_y, np.float32)\n",
    "valid_x = np.array(valid_x, np.float32)\n",
    "valid_y = np.array(valid_y, np.float32)\n",
    "test_x = np.array(test_x, np.float32)\n",
    "test_y = np.array(test_y, np.float32)\n",
    "\n",
    "\n",
    "# print(train_y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Option: Model Selection & Regularizer</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "# Xavier w/ regularizer\n",
    "# model = Sequential([\n",
    "#     Input(shape=(3,)),\n",
    "#     Dense(32, activation='relu', kernel_initializer=GlorotUniform(), kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     Dense(64, activation='relu', kernel_initializer=GlorotUniform(), kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     Dense(32, activation='relu', kernel_initializer=GlorotUniform(), kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     Dense(16, activation='relu', kernel_initializer=GlorotUniform(), kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     Dense(3, activation='softmax', kernel_initializer=GlorotUniform(), kernel_regularizer=regularizers.l2(0.01))\n",
    "# ])\n",
    "\n",
    "# Xavier w/o regularizer\n",
    "model = Sequential([\n",
    "    Input(shape=(3,)),\n",
    "    Dense(32, activation='relu', kernel_initializer=GlorotUniform()),\n",
    "    Dense(64, activation='relu', kernel_initializer=GlorotUniform()),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu', kernel_initializer=GlorotUniform()),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu', kernel_initializer=GlorotUniform()),\n",
    "    Dense(3, activation='softmax', kernel_initializer=GlorotUniform())\n",
    "])\n",
    "\n",
    "# Random Normal w/ regularizer\n",
    "# model = Sequential([\n",
    "#     Input(shape=(3,)),\n",
    "#     Dense(32, activation='relu', kernel_initializer=RandomNormal(), kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     Dense(64, activation='relu', kernel_initializer=RandomNormal(), kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     Dense(32, activation='relu', kernel_initializer=RandomNormal(), kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     Dense(16, activation='relu', kernel_initializer=RandomNormal(), kernel_regularizer=regularizers.l2(0.01)),\n",
    "#     Dense(3, activation='softmax', kernel_initializer=RandomNormal(), kernel_regularizer=regularizers.l2(0.01))\n",
    "# ])\n",
    "\n",
    "# Random Normal w/o regularizer\n",
    "# model = Sequential([\n",
    "#     Input(shape=(3,)),\n",
    "#     Dense(32, activation='relu', kernel_initializer=RandomNormal()), #\n",
    "#     Dense(64, activation='relu', kernel_initializer=RandomNormal()),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(32, activation='relu', kernel_initializer=RandomNormal()),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(16, activation='relu', kernel_initializer=RandomNormal()),\n",
    "#     Dense(3, activation='softmax', kernel_initializer=RandomNormal())\n",
    "# ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Option: Optimizer & Learning Rate</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers and Compiling\n",
    "\n",
    "# ADAM Optimizer w/ Learning Rate of 0.01\n",
    "# model.compile(optimizer=Adam(learning_rate=.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ADAM Optimizer w/ Learning Rate of 0.0001\n",
    "model.compile(optimizer=Adam(learning_rate=.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Momentum Optimizer w/ Learning Rate of 0.01\n",
    "# model.compile(optimizer=SGD(learning_rate=.01, momentum=0.7), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Momentum Optimizer w/ Learning Rate of 0.0001\n",
    "# model.compile(optimizer=SGD(learning_rate=.001, momentum=0.7), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ReduceLROnPlateau callback,hopes in reducing plateu in loss\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Option: Batch Size & Epochs</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "\n",
    "# Batch Size of 32, Epochs of 80\n",
    "history = model.fit(train_x, train_y, epochs=80, batch_size=32, validation_data=(valid_x, valid_y), callbacks=[reduce_lr])\n",
    "\n",
    "# Batch size of 100, epochs of 80\n",
    "# history = model.fit(train_x, train_y, epochs=80, batch_size=100, validation_data=(valid_x, valid_y), callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Evaluation\n",
    "test_loss, test_acc = model.evaluate(valid_x, valid_y)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Script to Save and Load Moel for Testing\n",
    "model.save('models/neural_network_model.keras') # save model\n",
    "loaded_model = load_model('models/neural_network_model.keras') # load model\n",
    "\n",
    "# Evaluate on validation set\n",
    "test_loss, test_acc = loaded_model.evaluate(valid_x, valid_y)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_x, test_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Results\n",
    "\n",
    "# Plot training, validation, and test accuracy values\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.plot([test_acc]*len(history.history['accuracy']), label='Test', linestyle='--')\n",
    "plt.title('Model Accuracy (Train, Validation, and Test)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot training and validation loss values\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot validation and test loss values\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.axhline(y=test_loss, color='r', linestyle='--', label='Test Loss')\n",
    "plt.title('Model Loss Progression (Validation and Test)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Results</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The following graphs are for the base run of the project. Throughout the training, I implemented learning rate reduction technique with the hopes of reducing plateus that occur during the learning process.  The structure of the run is as follows.\n",
    "\n",
    "- Run: Base\n",
    "- Normalization: Yes\n",
    "- Batch Size: 32\n",
    "- Epochs: 80\n",
    "- Learning Rate: 0.01\n",
    "- Optimizer: Adam\n",
    "- Initialization: Random\n",
    "- Regularization: No\n",
    "<h3>Graphs</h3>\n",
    "<img src=\"Images/Base Run Model Accuracy.png\">\n",
    "<img src=\"Images/Base Run Loss.png\">\n",
    "<img src=\"Images/Base Model Loss Progression.png\">\n",
    "\n",
    "<h2>Observations</h2>\n",
    "<h3>Accuracy</h3>\n",
    "Looking at the graph, it can be observed that the model's training accuracy steadily increases throughout each epoch, Indicating that the model is learning effectively from the data. It can also be observed that during the upward trend of the graph, the fluctuations becoem less and less pronounced. This indicates that the model is generalizing well to unseen validation data. By epoch 40, training and validation accuracies seems to stabalize, with the model acheiving around a 76% accuracy. \n",
    "<h3>Loss over Epochs</h3>\n",
    "In this graph, the training loss shows a consistent downward trend, indicating that the model is minimizing its errors over time. The validation loss decreases similarly, although with higher fluctiations during the early epochs. Since the gap between training and validation loss is not significant, it can be assumed that the model is not overfitting to the training data. \n",
    "<h3>Model Loss Progression</h3>\n",
    "In this graph, it can be observed that the validation loss decreases steadily, while the test loss remains constant. This suggests that the model has learned effectively from the training data and has generalized well to my unseen test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Run 2</h2>\n",
    "\n",
    "\n",
    "- Run: 2\n",
    "- Normalization: No\n",
    "- Batch Size: 32\n",
    "- Epochs: 80\n",
    "- Learning Rate: 0.01\n",
    "- Optimizer: Adam\n",
    "- Initialization: Random\n",
    "- Regularization: Yes\n",
    "<h3>Graphs</h3>\n",
    "<img src=\"Images/Run 2 Accuracy.png\">\n",
    "<img src=\"Images/Run 2 Loss.png\">\n",
    "<img src=\"Images/Run 2 Loss Progression.png\">\n",
    "\n",
    "<h2>Observations</h2>\n",
    "<h3>Accuracy</h3>\n",
    "The accuracy graph improves dramatically during the first few epochs. However, it seems to flatline after about 20 epochs, indicating that the model may not be learning effectively beyond a certain point. \n",
    "<h3>Loss over Epochs</h3>\n",
    "The training and validation losses improve steadily during the first few epochs, but plateu around 0.72. This may suggest that the learning has slowed down too early. \n",
    "<h3>Model Loss Progression</h3>\n",
    "The test loss flatlines at around 0.74. This suggests that this model configuration is not learning much from the training process after a certain point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2>Run 3</h2>\n",
    "\n",
    "- Run:3\n",
    "- Normalization: Yes\n",
    "- Batch Size: 100\n",
    "- Epochs: 80\n",
    "- Learning Rate: 0.0001\n",
    "- Optimizer: Momentum\n",
    "- Initialization:Xavier\n",
    "- Regularization:No\n",
    "<h3>Graphs</h3>\n",
    "<img src=\"Images/Run 3 Accuracy.png\">\n",
    "<img src=\"Images/Run 3 Loss.png\">\n",
    "<img src=\"Images/Run 3 Loss Progression.png\">\n",
    "\n",
    "<h2>Observations</h2>\n",
    "<h3>Accuracy</h3>\n",
    "In this run, the accuracy shows a noticable increase compared to the previous run. The validation accuracy increases rapidly, plateuing near 0.75, while the test accuracy remains stable at around 0.72. This suggests that the model is learning more effectively in this configuration.\n",
    "<h3>Loss over Epochs</h3>\n",
    "Similarly with the accuracy graph, the loss over epochs shows significant improvement for both training and validation. They both decrease steadily over the epochs with validation loss plateuing around 0.65. This indicates that the model is converging well with this learning rate. However, the diffrence between training and validation loss is still present. This may indicate potential overfitting. \n",
    "<h3>Model Loss Progression</h3>\n",
    "The test loss remains stable around 0.65, which suggests that the model is generalizing better to the test set with this configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Run 4\n",
    "</h2>\n",
    "\n",
    "\n",
    "- Run: 4\n",
    "- Normalization: No\n",
    "- Batch Size: 100\n",
    "- Epochs: 80\n",
    "- Learning Rate: 0.01\n",
    "- Optimizer: Momentum\n",
    "- Initialization: Xavier\n",
    "- Regularization: Yes\n",
    "<h3>Graphs</h3>\n",
    "<img src=\"Images/Run 4 Accuracy.png\">\n",
    "<img src=\"Images/Run 4 Loss.png\">\n",
    "<img src=\"Images/Run 4 Loss Progression.png\">\n",
    "\n",
    "<h2>Observations</h2>\n",
    "<h3>Accuracy</h3>\n",
    "This model's accuracy shows a steady increase and stabalizes around 68%. There is some fluctuation in the validation accuracy in early epochs, but both training and validation accuracy successfully converge. \n",
    "<h3>Loss over Epochs</h3>\n",
    "The training and validation losses decrese in the first 10 epochs, reaching lower and stable level. However, the test loss, which is computed after training, is represented by a horizontal line and remains higher than the validation and training losses. This might indicate generalization issues. \n",
    "<h3>Model Loss Progression</h3>\n",
    "The test loss remains relatively high, as shown by the horizontal line, suggesting that the model may not generalize well on onseen data. This may be due to overfitting, which could be mitigated by adjusting regularization, learning rate, or implementing strategies like early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Run 5\n",
    "</h2>\n",
    "\n",
    "\n",
    "- Run: 5\n",
    "- Normalization: Yes\n",
    "- Batch Size: 32\n",
    "- Epochs: 80\n",
    "- Learning Rate: 0.0001\n",
    "- Optimizer: Adam\n",
    "- Initialization: Xavier\n",
    "- Regularization: No\n",
    "<h3>Graphs</h3>\n",
    "<img src=\"\">\n",
    "<img src=\"\">\n",
    "<img src=\"\">\n",
    "\n",
    "<h2>Observations</h2>\n",
    "<h3>Accuracy</h3>\n",
    "\n",
    "<h3>Loss over Epochs</h3>\n",
    "\n",
    "<h3>Model Loss Progression</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "PlaceHolder\n",
    "</h2>\n",
    "\n",
    "\n",
    "- Run: \n",
    "- Normalization: \n",
    "- Batch Size: \n",
    "- Epochs: \n",
    "- Learning Rate: \n",
    "- Optimizer: \n",
    "- Initialization: \n",
    "- Regularization: \n",
    "<h3>Graphs</h3>\n",
    "<img src=\"\">\n",
    "<img src=\"\">\n",
    "<img src=\"\">\n",
    "\n",
    "<h2>Observations</h2>\n",
    "<h3>Accuracy</h3>\n",
    "\n",
    "<h3>Loss over Epochs</h3>\n",
    "\n",
    "<h3>Model Loss Progression</h3>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
